from collections import OrderedDict

from hw8.roble.critics.dqn_critic import DQNCritic
from hw8.roble.critics.cql_critic import CQLCritic
from hw8.roble.infrastructure.replay_buffer import ReplayBuffer
from hw8.roble.infrastructure.utils import *
from hw8.roble.policies.argmax_policy import ArgMaxPolicy
from hw8.roble.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer
from .dqn_agent import DQNAgent
import numpy as np
import ogbench


class ExplorationOrExploitationAgent(DQNAgent):
    
    def __init__(self, env, agent_params, normalize_rnd=True, rnd_gamma=0.99):

        # Load OGBench dataset into replay buffer
        if agent_params['alg']['offline_exploitation']:
            # Load dataset
            train_dataset_path = "/data/ogbench_data/visual-cube-triple-play-v0.npz"
            train_dataset = ogbench.load_dataset(
                train_dataset_path,
                ob_dtype=np.uint8,  # Observations are images (uint8)
                action_dtype=np.float32,  # Actions are continuous (float32)
                compact_dataset=False,
            )
            
            # Convert to paths compatible with replay buffer
            for traj in train_dataset:
                path = {
                    "observation": traj["observations"],
                    "action": traj["actions"],
                    "reward": traj["rewards"],
                    "next_observation": traj["next_observations"],
                    "terminal": traj["terminals"],
                }
                self.replay_buffer.store_path(path)

        super(ExplorationOrExploitationAgent, self).__init__(env, agent_params)

        print("DEBUG: agent_params['alg'] type =", type(agent_params['alg']))

        print("DEBUG: agent_params['alg'] type before deepcopy =", type(agent_params['alg']))

        # Ensure agent_params['alg'] remains a dictionary when passed to DQNAgent
        safe_agent_params = copy.deepcopy(agent_params)

        print("DEBUG: agent_params['alg'] type after deepcopy =", type(safe_agent_params['alg']))

        self.exploration_model = DQNAgent(self.env, safe_agent_params)

        #Ensure 'alg' is a dictionary
        if isinstance(agent_params["alg"], tuple):
            print("WARNING: Detected 'alg' as a tuple in explore_or_exploit_agent. Fixing...")
            agent_params["alg"] = agent_params["alg"][0]

        if 'ob_dim' not in agent_params or agent_params['ob_dim'] == 0:
            agent_params['ob_dim'] = self.env.observation_space.shape[0]

        if 'ac_dim' not in agent_params or agent_params['ac_dim'] == 0:
            agent_params['ac_dim'] = self.env.action_space.n

        if 'double_q' not in agent_params:
            agent_params['double_q'] = False  

        
        self.replay_buffer = MemoryOptimizedReplayBuffer(100000, 1, float_obs=True)
        self.num_exploration_steps = agent_params['alg']['num_exploration_steps']
        self.offline_exploitation = agent_params['alg']['offline_exploitation']

        self.exploitation_critic = CQLCritic(agent_params, self.optimizer_spec)
        self.exploration_critic = DQNCritic(**agent_params)
        
        self.exploration_model = DQNAgent(self.env, agent_params)
        self.explore_weight_schedule = agent_params['explore_weight_schedule']
        self.exploit_weight_schedule = agent_params['exploit_weight_schedule']
        
        self.actor = ArgMaxPolicy(self.exploration_critic)
        self.eval_policy = ArgMaxPolicy(self.exploitation_critic)
        self.exploit_rew_shift = agent_params['alg'].get('exploit_rew_shift',0.0)
        self.exploit_rew_scale = agent_params['alg'].get('exploit_rew_scale',0.0)
        self.eps = agent_params['alg'].get('eps',0.0)

        self.running_rnd_rew_std = 1
        self.normalize_rnd = normalize_rnd
        self.rnd_gamma = rnd_gamma

    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):
        log = {}

        if self.t > self.num_exploration_steps:
            # TODO: After exploration is over, set the actor to optimize the extrinsic critic
            #HINT: Look at method ArgMaxPolicy.set_critic
            self.actor.set_critic(self.exploitation_critic)
            self.replay_buffer.disable_collection()
        
        if not self.replay_buffer.can_sample(self.batch_size):
            print(f"WARNING: Replay Buffer has insufficient samples (needed: {self.batch_size})")
            return log


        if (self.t > self.learning_starts
                and self.t % self.learning_freq == 0
                and self.replay_buffer.can_sample(self.batch_size)
        ):

            print(f"DEBUG: Using Critic Type - {type(self.exploitation_critic)}")
            print("DEBUG: Calling CQL Critic Update")

            # Get Reward Weights
            # TODO: Get the current explore reward weight and exploit reward weight
            #       using the schedule's passed in (see __init__)
            # COMMENT: Until part 3, explore_weight = 1, and exploit_weight = 0
            explore_weight = self.explore_weight_schedule.value(self.t)
            exploit_weight = self.exploit_weight_schedule.value(self.t)

            # Run Exploration Model #
            # TODO: Evaluate the exploration model on s to get the exploration bonus
            # HINT: Normalize the exploration bonus, as RND values vary highly in magnitude.
            # HINT: Normalize using self.running_rnd_rew_std, and keep an exponential moving average
            # of self.running_rnd_rew_std using self.rnd_gamma.
            
            # Compute exploration bonus (RND)
            next_ob_tensor = ptu.from_numpy(next_ob_no)
            with torch.no_grad():
                target = self.exploration_model.target_network(next_ob_tensor)
                pred = self.exploration_model.predictor_network(next_ob_tensor)
                expl_bonus = torch.mean((target - pred) ** 2, dim=1).cpu().numpy()

            # Normalize bonus
            self.running_rnd_rew_std = self.rnd_gamma * self.running_rnd_rew_std + (1 - self.rnd_gamma) * np.std(expl_bonus)
            expl_bonus_normalized = expl_bonus / (self.running_rnd_rew_std + 1e-8)

            # Reward Calculations #
            # TODO: Calculate mixed rewards, which will be passed into the exploration critic
            # HINT: See doc for definition of mixed_reward

            mixed_reward = mixed_reward = explore_weight * expl_bonus_normalized + exploit_weight * re_n

            # TODO: Calculate the environment reward
            # HINT: For part 1, env_reward is just 're_n'
            #       After this, env_reward is 're_n' shifted by self.exploit_rew_shift,
            #       and scaled by self.exploit_rew_scale
            #shift = 1
            #scale = 100
            env_reward = (re_n + self.exploit_rew_shift) * self.exploit_rew_scale

            # Update Critics And Exploration Model #

            # TODO 1): Update the exploration model (based off s')
            # TODO 2): Update the exploration critic (based off mixed_reward)
            # TODO 3): Update the exploitation critic (based off env_reward)
            print("DEBUG: Calling CQL Critic Update")
            expl_model_loss = self.exploration_model.update(ob_no, ac_na, expl_bonus_normalized, next_ob_no, terminal_n)
            exploration_critic_loss = self.exploration_critic.update(ob_no, ac_na, next_ob_no, mixed_reward, terminal_n)
            exploitation_critic_loss = self.exploitation_critic.update(ob_no, ac_na, next_ob_no, env_reward, terminal_n)

            # Target Networks #
            if self.num_param_updates % self.target_update_freq == 0:
                # TODO: Update the exploitation and exploration target networks
                self.exploration_critic.update_target_network()
                self.exploitation_critic.update_target_network()


            # Logging #
            log['Exploitation Critic Loss'] = exploitation_critic_loss['Training Loss']
            log['Exploration Critic Loss'] = exploration_critic_loss['Training Loss']
            log['Exploration Model Loss'] = expl_model_loss

            # TODO: Uncomment these lines after completing cql_critic.py
            if self.exploitation_critic.cql_alpha >= 0:
                log['Exploitation Data q-values'] = exploitation_critic_loss['Data q-values']
                log['Exploitation OOD q-values'] = exploitation_critic_loss['OOD q-values']
                log['Exploitation CQL Loss'] = exploitation_critic_loss['CQL Loss']
            
            if "Max Q-value" in exploitation_critic_loss:
                log["Max Q-value"] = exploitation_critic_loss["Max Q-value"]

            self.num_param_updates += 1

        self.t += 1
        return log


    def step_env(self):
        """
            Step the env and store the transition
            At the end of this block of code, the simulator should have been
            advanced one step, and the replay buffer should contain one more transition.
            Note that self.last_obs must always point to the new latest observation.
        """
        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs)

        perform_random_action = np.random.random() < self.eps or self.t < self.learning_starts

        if perform_random_action:
            action = self.env.action_space.sample()
        else:
            processed = self.replay_buffer.encode_recent_observation()
            action = self.actor.get_action(processed)

        next_obs, reward, done, info = self.env.step(action)
        self.last_obs = next_obs.copy()

        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.env.reset()
