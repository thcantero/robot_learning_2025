env:
    env_name: 'MsPacman-v0' # ['LunarLander-v3', 'MsPacman-v0', 'HalfCheetah-v2', 'CartPole-v0']
    max_episode_length: 200
    exp_name: 'q1'
    atari: False

alg:
    double_q: False #False
    batch_size: 64 ## The min amount of experience to collect before a training update
    train_batch_size: 64 ## training batch size used for computing gradients of q function or policy
    eval_batch_size: 4096 #1000 ## How much experience should be collected over the environment to evaluate the average reward of a policy
    num_agent_train_steps_per_iter: 2 ## Number of training updates after #batch_size experience is collected. 
    num_critic_updates_per_agent_update: 2 ## Number of training updates after #batch_size experience is collected.
    no_gpu: false #use_gpu: False
    which_gpu: 0 #gpu_id: 0
    rl_alg: 'dqn' ## RL training algorithm ['dqn', 'ddpg', 'td3', 'sac']
    use_rnd: true # missing
    num_exploration_steps: 10000
    unsupervised_exploration: true
    learning_starts: 10000  ## How much initial experience to collect before training begins
    learning_freq: 1 
    target_update_freq: 10000
    exploration_schedule: 0
    exploration_initial_eps: 1.0      # Start with 100% random actions
    exploration_final_eps: 0.01       # End with 1% random actions
    exploration_decay_steps: 1e6      # Decay Îµ over 1M steps
    replay_buffer_size: 1000000
    frame_history_len: 4
    gamma: 0.99
    optimizer_spec:
        constructor: "Adam"
        optim_kwargs:
            lr: 0.001
            eps: 1e-4
        learning_rate_schedule: "lambda t: 1.0"
    critic_learning_rate: 1e-3
    learning_rate: 1e-4
    ob_dim: 0             # do not modify
    ac_dim: 0             # do not modify
    batch_size_initial: 0 # do not modify
    discrete: True
    grad_norm_clipping: True
    n_iter: 10000 #50_000_000
    polyak_avg: 0.01 #
    td3_target_policy_noise: 0.05 #
    td3_target_policy_noise_clip: 0.1 #
    entropy_coeff: 0.2 ## SAC entropy coeff
    policy_std: 0.1
    nn_baseline: False
    deterministic: True
    network:
        layer_sizes:  [ 64, 64 ]
        activations: [ "tanh", "tanh" ]
        output_activation: "identity"

logging:
    video_log_freq: -1 # How often to generate a video to log/
    scalar_log_freq: 10000 # How often to log training information and run evaluation during training.
    save_params: false # Should the parameters given to the script be saved? (Always...)
    random_seed: 1234
    logdir: ""
    debug: false
